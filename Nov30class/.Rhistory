setwd("C:/Users/gehmand/OneDrive - Carlisle Area School District/Desktop/Data 180 dTomGehman/DATA180/Nov30class") # Set this to your local GitHub repository.
library(tm) # text mining package
# Instead of that, just read in the speech
charVector <- scan("speech.txt", character(0), sep = "\n") # "\n is a line separator
#############
# Text Mining
# nomination speech
setwd("C:/Users/gehmand/OneDrive - Carlisle Area School District/Desktop/Data 180 dTomGehman/DATA180/Nov30class") # Set this to your local GitHub repository.
library(tm) # text mining package
# Instead of that, just read in the speech
charVector <- scan("speech.txt", character(0), sep = "\n") # "\n is a line separator
head(charVector)
charVector_block <- scan("speech.txt", character(0))
charVector_block # default separator for the scan function is space.
# Which other separators are there?
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# cleaning part!!
wordVector <- VectorSource(charVector) # from tm package, convert to vector
class(wordVector); typeof(wordVector); length(wordVector)
wordCorpus <- Corpus(wordVector) # convert to corpus
class(wordCorpus); typeof(wordCorpus); length(wordCorpus)
# first step transformation: make all of the letters in "wordCorpus" lowercase
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
# second step transformation: remove the punctuation in "wordCorpus"
wordCorpus <- tm_map(wordCorpus, removePunctuation)
# third step transformation: remove numbers in "wordCorpus"
wordCorpus <- tm_map(wordCorpus, removeNumbers)
# final step transformation: take out the "stop" words, such as "the", "a" and "at"
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
stop_words<-stopwords("english")
stop_words
wordCorpus[["1"]][["content"]]
tdm <- TermDocumentMatrix(wordCorpus)
# view term-document matrix "tdm"
tdm
fTerms <- findFreqTerms(tdm, lowfreq = 20) # from tm
fTerms
# Common terms: "tonight"  "new"      "people"   "children" "family"   "will"     "families" "america"
findAssocs(tdm, fTerms, 0.4) # for fTerms, what comes next?
Zipf_plot(tdm)
# Create a list of counts for each word
# convert tdm into a matrix called "m"
m <- as.matrix(tdm)
dim(m) # 1210 x 166, 1210 unique words
# get total count for each word
wordCounts <- rowSums(m)
# sort words in "wordCounts" by frequency
wordCounts <- sort(wordCounts, decreasing=TRUE)
# check the first several items in "wordCounts" to see if it is built correctly
head(wordCounts)
totalWords <- sum(wordCounts) # total occurance of words
barplot(wordCounts)
barplot(table(wordCounts))
Zipf_plot(tdm)
#install.packages("spacyr") # should only need to do this once.
library("spacyr")
spacy_install()
#if things give error, or get stuck, try
library("reticulate")
reticulate::conda_install(envname = "spacy_condaenv", packages = 'spacy')
spacy_install()
reticulate::conda_install(envname = "spacy_condaenv", packages = 'spacy')
#install.packages("spacyr") # should only need to do this once.
library("spacyr")
spacy_initialize() # Start the spacy session
